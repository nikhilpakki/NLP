{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc81493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0582994a3f40457e85763eb26233ecf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_2_2\n",
      "C:\\Users\\Nikhil\\.conda\\envs\\Conda_NLP\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af624c9da02a4d38a391d0788251c939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526bd24798f04da088c6c4a89d2e3218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b28e6cd342480fb6df49742605ecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_2_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aa8b8330f043d5be3f0dc6ddc069a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 0, 'tn': 1, 'fp': 0, 'fn': 1, 'auroc': 1.0, 'auprc': 1.0, 'eval_loss': 0.6978808641433716}\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbe9fcb141e43058c9e4f2aa4b37987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fc0993a07647fabdb9f082800f6e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Preparing train data\n",
    "train_data = [\n",
    "    [\"Aragorn was the heir of Isildur\", 1],\n",
    "    [\"Frodo was the heir of Isildur\", 0],\n",
    "]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "# Preparing eval data\n",
    "eval_data = [\n",
    "    [\"Theoden was the king of Rohan\", 1],\n",
    "    [\"Merry was the king of Rohan\", 0],\n",
    "]\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(num_train_epochs=1)\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "# Make predictions with the model\n",
    "predictions, raw_outputs = model.predict([\"Sam was a Wizard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef31dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb2a8424da24aed903fe8b6e420b1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6490e75347b4614861756beed970bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), array([[ 0.19162971, -0.02407446]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"Sam was a Wizard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55b3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff88e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43d60a38",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e56f4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm \n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b5fe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    ('Who is Nishanth?', {\n",
    "        'entities': [(7, 15, 'PERSON2')]\n",
    "    }),\n",
    "     ('Who is Kamal Khumar?', {\n",
    "        'entities': [(7, 19, 'PERSON2')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6417d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "output_dir=Path(\"C:\\\\Users\\\\nithi\\\\Documents\\\\ner\")\n",
    "n_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6f2b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9ab8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3c03770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-08 17:30:22,051] [INFO] Created vocabulary\n",
      "INFO:spacy:Created vocabulary\n",
      "[2022-02-08 17:30:22,053] [INFO] Finished initializing nlp object\n",
      "INFO:spacy:Finished initializing nlp object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 13.499998271465302}\n",
      "{'ner': 13.200329780578613}\n",
      "{'ner': 12.834150552749634}\n",
      "{'ner': 12.526989877223969}\n",
      "{'ner': 11.870877385139465}\n",
      "{'ner': 11.379868686199188}\n",
      "{'ner': 10.592233419418335}\n",
      "{'ner': 10.132927060127258}\n",
      "{'ner': 8.62405788898468}\n",
      "{'ner': 8.000928103923798}\n",
      "{'ner': 6.703537315130234}\n",
      "{'ner': 6.52680104970932}\n",
      "{'ner': 5.166276719421148}\n",
      "{'ner': 4.119650658220053}\n",
      "{'ner': 4.598843222483993}\n",
      "{'ner': 3.7226006558630615}\n",
      "{'ner': 4.762943888315931}\n",
      "{'ner': 3.3442667427298147}\n",
      "{'ner': 2.9585678041621577}\n",
      "{'ner': 2.8887376190832583}\n",
      "{'ner': 8.073645882454002}\n",
      "{'ner': 5.528188198251883}\n",
      "{'ner': 3.5133605854789494}\n",
      "{'ner': 5.208720222064585}\n",
      "{'ner': 3.09231027147689}\n",
      "{'ner': 3.8505216681514867}\n",
      "{'ner': 3.905920750388759}\n",
      "{'ner': 3.350601361536974}\n",
      "{'ner': 2.6630238850339083}\n",
      "{'ner': 1.9381053008255549}\n",
      "{'ner': 2.538483471464133}\n",
      "{'ner': 1.6183970494021196}\n",
      "{'ner': 0.8579220693791285}\n",
      "{'ner': 0.7020434782170923}\n",
      "{'ner': 0.47077013272064505}\n",
      "{'ner': 0.4762850091792643}\n",
      "{'ner': 0.4131426966196159}\n",
      "{'ner': 0.32669037503001164}\n",
      "{'ner': 0.19423817115966813}\n",
      "{'ner': 0.23102097550327017}\n",
      "{'ner': 0.14436273945466382}\n",
      "{'ner': 0.04452312770536082}\n",
      "{'ner': 0.06262923835720358}\n",
      "{'ner': 0.05085968341882108}\n",
      "{'ner': 0.006097976402756444}\n",
      "{'ner': 0.038518842332507575}\n",
      "{'ner': 0.0055510542273253805}\n",
      "{'ner': 0.0004273553852272016}\n",
      "{'ner': 0.00040914804273262106}\n",
      "{'ner': 2.7577911158260804e-05}\n",
      "{'ner': 1.616332685645816e-05}\n",
      "{'ner': 0.00013582224502484141}\n",
      "{'ner': 1.7059838575139707e-07}\n",
      "{'ner': 4.468640277144331e-06}\n",
      "{'ner': 2.3484062765130598e-05}\n",
      "{'ner': 9.593040041205175e-07}\n",
      "{'ner': 7.78066424602067e-05}\n",
      "{'ner': 8.804741380430414e-08}\n",
      "{'ner': 1.1437140648440369e-07}\n",
      "{'ner': 5.956078954691413e-08}\n",
      "{'ner': 2.785903057819436e-07}\n",
      "{'ner': 2.7911160947032825e-08}\n",
      "{'ner': 7.126788018406536e-10}\n",
      "{'ner': 6.582689377528789e-09}\n",
      "{'ner': 1.1073851836603324e-08}\n",
      "{'ner': 2.3564330788841854e-08}\n",
      "{'ner': 1.2972249659485777e-07}\n",
      "{'ner': 6.4273886760859295e-06}\n",
      "{'ner': 1.5557740287025501e-09}\n",
      "{'ner': 7.358459232567616e-09}\n",
      "{'ner': 1.953520965163343e-08}\n",
      "{'ner': 1.316813999866781e-08}\n",
      "{'ner': 1.1190935512655169e-07}\n",
      "{'ner': 1.2930522991314725e-09}\n",
      "{'ner': 7.967173650414577e-10}\n",
      "{'ner': 1.3182560920361845e-08}\n",
      "{'ner': 1.572246700983404e-08}\n",
      "{'ner': 2.6116000802716257e-08}\n",
      "{'ner': 0.00039106585611533537}\n",
      "{'ner': 5.581333748132049e-09}\n",
      "{'ner': 1.1190471376474644e-08}\n",
      "{'ner': 1.4919445089946193e-08}\n",
      "{'ner': 1.2988726995119397e-08}\n",
      "{'ner': 2.7475069015633435e-05}\n",
      "{'ner': 2.0274806994966324e-09}\n",
      "{'ner': 4.946462755023019e-08}\n",
      "{'ner': 2.5107528274330437e-10}\n",
      "{'ner': 4.765661226445992e-08}\n",
      "{'ner': 4.249384920546398e-11}\n",
      "{'ner': 8.798450076395081e-10}\n",
      "{'ner': 7.795188842624212e-09}\n",
      "{'ner': 5.909480115832494e-09}\n",
      "{'ner': 2.9666917347877345e-09}\n",
      "{'ner': 3.64771459894107e-08}\n",
      "{'ner': 1.619376125019307e-09}\n",
      "{'ner': 6.729403964541312e-10}\n",
      "{'ner': 3.2694537811374664e-10}\n",
      "{'ner': 0.0007371918426782738}\n",
      "{'ner': 2.7567307835357074e-09}\n",
      "{'ner': 8.267997973770096e-07}\n"
     ]
    }
   ],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        example = []\n",
    "        texts, annotations = zip(*TRAIN_DATA)\n",
    "        # Update the model with iterating each text\n",
    "        for i in range(len(texts)):\n",
    "            doc = nlp.make_doc(texts[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "        nlp.update(example, losses=losses, drop=0.3)\n",
    "        '''\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            nlp.update(\n",
    "                text,  annotations,  \n",
    "                drop=0.5,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        '''\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "183b5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Kamal Khumar', 'PERSON2')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Kamal', 'PERSON2', 3), ('Khumar', 'PERSON2', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Entities [('Nishanth', 'PERSON2')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Nishanth', 'PERSON2', 3), ('?', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46794ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f18105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load('en_core_web_lg')\n",
    "#spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e594a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx1 = nlp1(u\"Dupixent significantly improved skin clearance and reduced overall disease severity and itch in pivotal trial that met all primary and secondary endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f924311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupixent 0 8 ORG\n"
     ]
    }
   ],
   "source": [
    "for token in docx1.ents:\n",
    "    print(token.text,token.start_char, token.end_char,token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecb402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3722d74f",
   "metadata": {},
   "source": [
    "### Sequence 2 Sequence modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac75e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba7e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2800b90b944bbcb2f30243ae90f6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2b13dc548747848dc1b97cb164cec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca599802301b4de389b42e61e31d6534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2fddf356ea4a888fd6e2c8efd3cc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f9b34be0494551a817885cfb79bbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Output directory (outputs/) already exists and is not empty. Set args.overwrite_output_dir = True to overcome.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_90960/2092524613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Conda_NLP\\lib\\site-packages\\simpletransformers\\seq2seq\\seq2seq_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite_output_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         ):\n\u001b[1;32m--> 435\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    436\u001b[0m                 \u001b[1;34m\"Output directory ({}) already exists and is not empty.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;34m\" Set args.overwrite_output_dir = True to overcome.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Output directory (outputs/) already exists and is not empty. Set args.overwrite_output_dir = True to overcome."
     ]
    }
   ],
   "source": [
    "train_data = [\n",
    "    [\n",
    "        \"Perseus “Percy” Jackson is the main protagonist and the narrator of the Percy Jackson and the Olympians series.\",\n",
    "        \"Percy is the protagonist of Percy Jackson and the Olympians\",\n",
    "    ],\n",
    "    [\n",
    "        \"Annabeth Chase is one of the main protagonists in Percy Jackson and the Olympians.\",\n",
    "        \"Annabeth is a protagonist in Percy Jackson and the Olympians.\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "eval_data = [\n",
    "    [\n",
    "        \"Grover Underwood is a satyr and the Lord of the Wild. He is the satyr who found the demigods Thalia Grace, Nico and Bianca di Angelo, Percy Jackson, Annabeth Chase, and Luke Castellan.\",\n",
    "        \"Grover is a satyr who found many important demigods.\",\n",
    "    ],\n",
    "    [\n",
    "        \"Thalia Grace is the daughter of Zeus, sister of Jason Grace. After several years as a pine tree on Half-Blood Hill, she got a new job leading the Hunters of Artemis.\",\n",
    "        \"Thalia is the daughter of Zeus and leader of the Hunters of Artemis.\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "# Configure the model\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    \"roberta\",\n",
    "    \"roberta-base\",\n",
    "    \"bert-base-cased\",\n",
    "    args=model_args, use_cuda=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, eval_data=eval_df, args.overwrite_output_dir=True)\n",
    "\n",
    "# Evaluate the model\n",
    "result = model.eval_model(eval_df)\n",
    "\n",
    "# Use the model for prediction\n",
    "print(\n",
    "    model.predict(\n",
    "        [\n",
    "            \"Tyson is a Cyclops, a son of Poseidon, and Percy Jackson’s half brother. He is the current general of the Cyclopes army.\"\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
